# Machine_Learning

This repository showcases practical examples, shedding light on machine learning by investigating different algorithms.

## Example 1: Breast Cancer Prediction by Logistic regression

## Example 2: Dielectric Performance Prediction by Non-linear regression

Dielectrics are used in capacitors and semiconductor devices, hold immense promise for novel applications like artificial muscles. Furthermore, predicting their failure mode is of great importance. 
To address this issue, a generic example named Example_2 is presented:
### Key Steps:
 1. Assessing Performance: A comprehensive dataset is employed to assess dielectric performance.
 2. Prediction Approach: A non-linear regression method predicts material performance based on the maximum voltage applied to dielectrics.
 3. Educational Purpose: This example serves as a basic tutorial for students, introducing them to regression approaches.
 4. Code Reference: The Jupyter notebook “Nonlinear_Regression.ipynb” contains the implementation.
 5. Three different Kernel are implemented: RBF (Radial Basis Function), Second-order polynomial kernel and Sigmoid kernels
 6. L2-Regularization is applied to the models.
Moreover, they are implemented in a straightforward manner. Therefore, it’s useful to explore them through a basic example.

## Example 3: Classification of Perennial Plants Using K-Nearest Neighbors (KNN)
#### Dataset: "Iris.csv"

Classifying various plants based on their features such as their sepal width, sepal length, petal width, and petal length is a generic example artificial intelligence which is helpful for the beginners.
In  Example_3, K-Nearest Neighbors (KNN) is used to illustrate non-parametric algorithms' simplicity as well as their flexibility. 
The K-Nearest Neighbors (KNN) algorithm will be applied, and a confusion matrix will be generated to evaluate model performance.
### Key Steps
 1. Normalization: Data normalization will be performed to ensure that all features contribute equally to the distance calculation.
			Genre and Popularity Prediction for Music Dataset
 2. Data Preprocessing: The “name,” “year,” and “artist” columns will be removed from the music dataset.
			The “popularity” column will be split into five sections (e.g., low, moderate, high, very high, extremely high).
 3. Quantification and One-Hot Encoding: A new column called “new_genre” was created and filled base on one-hot encoding method.
 4. Data Features’ Distribution: The distribution of features in the dataset will be visualized to gain insights.
 5. Normalization and Correlations: Feature normalization will be applied, and correlations between different features will be explored.
 6. Data Splitting: The data will be split into training, validation, and test sets.
 7. KNN Algorithm: The KNN algorithm will be employed to predict genres based on the features.
			Hyperparameters will be tuned to find the best model in terms of accuracy.
 8. Reporting Genres: The most and least frequent genres in the dataset will be reported.
### Results

## Example 4: Classification of musical releases Using different algorithms (KNN)
#### Dataset: "musics.csv"
Record companies tend to classify their records by AI methods. Such action benefits them by enhancing content recommendation, personalization, metadata management, etc. 
In Example_4, a comprehensive study was adopted to classify released songs from 2000 to 2020. At the end, the accuracy of each algorithm was evaluated and reported.

### Key Steps
 1. Normalization: Data normalization will be performed to ensure that all features contribute equally to the distance calculation.
			Genre and Popularity Prediction for Music Dataset
 2. Data Preprocessing: The “name,” “year,” and “artist” columns will be removed from the music dataset.
			The “popularity” column will be split into five sections (e.g., low, moderate, high, very high, extremely high).
 3. Quantification and One-Hot Encoding: A new column called “new_genre” was created and filled base on one-hot encoding method.
 4. Data Features’ Distribution: The distribution of features in the dataset will be visualized to gain insights.
 5. Normalization and Correlations: Feature normalization will be applied, and correlations between different features will be explored.
 6. Data Splitting: The data will be split into training, validation, and test sets.
 7. Algorithms: Decision Tree, Random Forest, KNN and SVM are employed.
 8. Reporting Genres: The most and least frequent genres in the dataset will be reported.
### Results
